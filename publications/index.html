<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Yunsu  Kim


  | publications

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üçî</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Yunsu</span>   Kim
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kim2020when" class="col-sm-8">
    
      <div class="title">When and Why is Unsupervised Neural Machine Translation Useless?</div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Miguel Gra√ßa,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation (EAMT 2020)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2004.10581.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
      
      <a href="/assets/pdf/EAMT_2020_Oral.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
      <a href="https://slideslive.com/38929842/when-and-why-is-unsupervised-neural-machine-translation-useless" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper studies the practicality of the current state-of-the-art unsupervised methods in neural machine translation (NMT). In ten translation tasks with various data settings, we analyze the conditions under which the unsupervised methods fail to produce reasonable translations. We show that their performance is severely affected by linguistic dissimilarity and domain mismatch between source and target monolingual data. Such conditions are common for low-resource language pairs, where unsupervised learning works poorly. In all of our experiments, supervised and semi-supervised baselines with 50k-sentence bilingual data outperform the best unsupervised results. Our analyses pinpoint the limits of the current unsupervised NMT and also suggest immediate research directions.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kim2019pivot" class="col-sm-8">
    
      <div class="title">Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages</div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Petre Petrov,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Pavel Petrushkov,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Shahram Khadivi,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/D19-1080.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
      
      <a href="/assets/pdf/EMNLP_2019_Poster_final.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present effective pre-training strategies for neural machine translation (NMT) using parallel corpora involving a pivot language, i.e., source-pivot and pivot-target, leading to a significant improvement in source-target translation. We propose three methods to increase the relation among source, pivot, and target languages in the pre-training: 1) step-wise training of a single model for different language pairs, 2) additional adapter component to smoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder training via autoencoding of the pivot language. Our methods greatly outperform multilingual models up to +2.6% BLEU in WMT 2019 French-German and German-Czech tasks. We show that our improvements are valid also in zero-shot/zero-resource scenarios.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kim2019when" class="col-sm-8">
    
      <div class="title">When and Why is Document-level Context Useful in Neural Machine Translation?</div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Duc Thanh Tran,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/D19-6503.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/ducthanhtran/sockeye_document_context" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/DiscoMT_2019_Poster_final.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="/assets/pdf/DiscoMT_2019_Spotlight.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Document-level context has received lots of attention for compensating neural machine translation (NMT) of isolated sentences. However, recent advances in document-level NMT focus on sophisticated integration of the context, explaining its improvement with only a few selected examples or targeted test sets. We extensively quantify the causes of improvements by a document-level model in general test sets, clarifying the limit of the usefulness of document-level context in NMT. We show that most of the improvements are not interpretable as utilizing the context. We also show that a minimal encoding is sufficient for the context modeling and very long context is not helpful for NMT.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="graca2019generalizing" class="col-sm-8">
    
      <div class="title">Generalizing Back-Translation in Neural Machine Translation</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Miguel Gra√ßa,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Julian Schamper,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Shahram Khadivi,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the Fourth Conference on Machine Translation (WMT 2019)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/W19-5205.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
      
      <a href="/assets/pdf/slides_wmt_2019.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Back-translation ‚Äî data augmentation by translating target monolingual data ‚Äî is a crucial component in modern neural machine translation (NMT). In this work, we reformulate back-translation in the scope of cross-entropy optimization of an NMT model, clarifying its underlying mathematical assumptions and approximations beyond its heuristic usage. Our formulation covers broader synthetic data generation schemes, including sampling from a target-to-source NMT model. With this formulation, we point out fundamental problems of the sampling-based approaches and propose to remedy them by (i) disabling label smoothing for the target-to-source model and (ii) sampling from a restricted search space. Our statements are investigated on the WMT 2018 German &lt;-&gt; English news translation task.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="rosendahl2019rwth" class="col-sm-8">
    
      <div class="title">The RWTH Aachen University Machine Translation Systems for WMT 2019</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Jan Rosendahl,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Christian Herold,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Miguel Gra√ßa,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Weiyue Wang,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Parnia Bahar,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Yingbo Gao,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the Fourth Conference on Machine Translation (WMT 2019)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/W19-5338.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes the neural machine translation systems developed at the RWTH Aachen University for the German-English, Chinese-English and Kazakh-English news translation tasks of the Fourth Conference on Machine Translation (WMT19). For all tasks, the final submitted system is based on the Transformer architecture. We focus on improving data filtering and fine-tuning as well as systematically evaluating interesting approaches like unigram language model segmentation and transfer learning. For the De-En task, none of the tested methods gave a significant improvement over last years winning system and we end up with the same performance, resulting in 39.6% BLEU on newstest2019. In the Zh-En task, we show 1.3% BLEU improvement over our last year‚Äôs submission, which we mostly attribute to the splitting of long sentences during translation. We further report results on the Kazakh-English task where we gain improvements of 11.1% BLEU over our baseline system. On the same task we present a recent transfer learning approach, which uses half of the free parameters of our submission system and performs on par with it.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kim2019learning" class="col-sm-8">
    
      <div class="title">Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron</div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Hendrik Rosendahl,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Nick Rossenbach,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Jan Rosendahl,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Shahram Khadivi,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the Fourth Workshop on Representation Learning for NLP (RepL4NLP 2019)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/W19-4309.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
      
      <a href="/assets/pdf/poster_repl4nlp_2019.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a novel model architecture and training algorithm to learn bilingual sentence embeddings from a combination of parallel and monolingual data. Our method connects autoencoding and neural machine translation to force the source and target sentence embeddings to share the same space without the help of a pivot language or an additional transformation. We train a multilayer perceptron on top of the sentence embeddings to extract good bilingual sentence pairs from nonparallel or noisy parallel data. Our approach shows promising performance on sentence alignment recovery and the WMT 2018 parallel corpus filtering tasks with only a single model.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kim2019effective" class="col-sm-8">
    
      <div class="title">Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies</div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Yingbo Gao,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/P19-1120.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/yunsukim86/sockeye-transfer" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_acl_2019.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Transfer learning or multilingual model is essential for low-resource neural machine translation (NMT), but the applicability is limited to cognate languages by sharing their vocabularies. This paper shows effective techniques to transfer a pretrained NMT model to a new, unrelated language without shared vocabularies. We relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pretraining data without back-translation. Our methods do not require restructuring the vocabulary or retraining the model. We improve plain NMT transfer by up to +5.1% BLEU in five low-resource translation tasks, outperforming multilingual joint training by a large margin. We also provide extensive ablation studies on pretrained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of NMT transfer.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kim2018improving" class="col-sm-8">
    
      <div class="title">Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder</div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Jiahui Geng,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/D18-1101.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/yunsukim86/wbw-lm" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
      
      <a href="/assets/pdf/slides_emnlp_2018.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Unsupervised learning of cross-lingual word embedding offers elegant matching of words across languages, but has fundamental limitations in translating sentences. In this paper, we propose simple yet effective methods to improve word-by-word translation of cross-lingual embeddings, using only monolingual corpora but without any back-translation. We integrate a language model for context-aware search, and use a novel denoising autoencoder to handle reordering. Our system surpasses state-of-the-art unsupervised translation systems without costly iterative training. We also analyze the effect of vocabulary size and denoising type on the translation performance, which provides better understanding of learning the cross-lingual word embedding and its usage in translation.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="schamper2018rwth" class="col-sm-8">
    
      <div class="title">The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Julian Schamper,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Jan Rosendahl,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Parnia Bahar,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Arne Nix,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the Third Conference on Machine Translation (WMT 2018)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/W18-6426.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes the statistical machine translation systems developed at RWTH Aachen University for the German‚ÜíEnglish, English‚ÜíTurkish and Chinese‚ÜíEnglish translation tasks of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use ensembles of neural machine translation systems based on the Transformer architecture. Our main focus is on the German‚ÜíEnglish task where we to all automatic scored first with respect metrics provided by the organizers. We identify data selection, fine-tuning, batch size and model dimension as important hyperparameters. In total we improve by 6.8% BLEU over our last year‚Äôs submission and by 4.8% BLEU over the winning system of the 2017 German‚ÜíEnglish task. In English‚ÜíTurkish task, we show 3.6% BLEU improvement over the last year‚Äôs winning system. We further report results on the Chinese‚ÜíEnglish task where we improve 2.2% BLEU on average over our baseline systems but stay behind the 2018 winning systems.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="graca2018rwth" class="col-sm-8">
    
      <div class="title">The RWTH Aachen University English-German and German-English Unsupervised Neural Machine Translation Systems for WMT 2018</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Miguel Gra√ßa,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Julian Schamper,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Jiahui Geng,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the Third Conference on Machine Translation (WMT 2018)</em>
      
      </div>

      <div class="periodical">
      
        <b>Ranked 1st in English‚ÜíGerman and German‚ÜíEnglish tasks</b>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/W18-6409.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
      
      <a href="/assets/pdf/poster_wmt_2018_unsup.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes the unsupervised neural machine translation (NMT) systems of the RWTH Aachen University developed for the English ‚Üî German news translation task of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). Our work is based on iterative back-translation using a shared encoder-decoder NMT model. We extensively compare different vocabulary types, word embedding initialization schemes and optimization methods for our model. We also investigate gating and weight normalization for the word embedding layer.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="rossenbach2018rwth" class="col-sm-8">
    
      <div class="title">The RWTH Aachen University Filtering System for the WMT 2018 Parallel Corpus Filtering Task</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Nick Rossenbach,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Jan Rosendahl,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Miguel Gra√ßa,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Aman Gokrani,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the Third Conference on Machine Translation (WMT 2018)</em>
      
      </div>

      <div class="periodical">
      
        <b>Ranked 1st, 2nd, 5th, 5th in four tracks</b>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/W18-6487.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes the submission of RWTH Aachen University for the De‚ÜíEn parallel corpus filtering task of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use several rule-based, heuristic methods to preselect sentence pairs. These sentence pairs are scored with count-based and neural systems as language and translation models. In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic. Our best performing corpus filtering system relies on recurrent neural language models and translation models based on the transformer architecture. A model trained on 10M randomly sampled tokens reaches a performance of 9.2% BLEU on newstest2018. Using our filtering and ranking techniques we achieve 34.8% BLEU.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kim2017unsupervised" class="col-sm-8">
    
      <div class="title">Unsupervised Training for Large Vocabulary Translation Using Sparse Lexicon and Word Classes</div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Julian Schamper,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/E17-2103.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
      
      <a href="/assets/pdf/poster_eacl_2017.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We address for the first time unsupervised training for a translation task with hundreds of thousands of vocabulary words. We scale up the expectation-maximization (EM) algorithm to learn a large translation table without any parallel text or seed lexicon. First, we solve the memory bottleneck and enforce the sparsity with a simple thresholding scheme for the lexicon. Second, we initialize the lexicon training with word classes, which efficiently boosts the performance. Our methods produced promising results on two large-scale unsupervised translation tasks.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kim2016comparative" class="col-sm-8">
    
      <div class="title">A Comparative Study on Vocabulary Reduction for Phrase Table Smoothing</div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Andreas Guta,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Joern Wuebker,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hermann Ney
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the First Conference on Machine Translation (WMT 2016)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://aclanthology.org/W16-2212.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
      
      <a href="/assets/pdf/slides_wmt_2016.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This work systematically analyzes the smoothing effect of vocabulary reduction for phrase translation models. We extensively compare various word-level vocabularies to show that the performance of smoothing is not significantly affected by the choice of vocabulary. This result provides empirical evidence that the standard phrase translation model is extremely sparse. Our experiments also reveal that vocabulary reduction is more effective for smoothing large-scale phrase tables.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="hassani2015subspace" class="col-sm-8">
    
      <div class="title">Subspace Clustering of Data Streams: New Algorithms and Effective Evaluation Measures</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Marwan Hassani,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Seungjin Choi,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Thomas Seidl
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Intelligent Information Systems 45 (3)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://link.springer.com/article/10.1007/s10844-014-0319-2" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Nowadays, most streaming data sources are becoming high dimensional. Accordingly, subspace stream clustering, which aims at finding evolving clusters within subgroups of dimensions, has gained a significant importance. However, in spite of the rich literature of subspace and projected clustering algorithms on static data, only three stream projected algorithms are available. Additionally, existing subspace clustering evaluation measures are mainly designed for static data, and cannot reflect the quality of the evolving nature of data streams. On the other hand, available stream clustering evaluation measures care only about the errors of the full-space clustering but not the quality of subspace clustering. In this article we present a method for designing new stream subspace and projected algorithms. We propose also, to the first of our knowledge, the first subspace clustering measure that is designed for streaming data, called SubCMM: Subspace Cluster Mapping Measure. SubCMM is an effective evaluation measure for stream subspace clustering that is able to handle errors caused by emerging, moving, or splitting subspace clusters. Additionally, we propose a novel method for using available offline subspace clustering measures for data streams over the suggested new algorithms within the Subspace MOA framework.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kim2015phrase" class="col-sm-8">
    
      <div class="title">Phrase Table Smoothing with Word Classes</div>
      <div class="author">
        
          
          
          
          

          
            
              <em>Yunsu Kim</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>Master's Thesis, Fakult√§t f√ºr Mathematik, Informatik und Naturwissenschaften, RWTH Aachen University</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="/assets/pdf/mscthesis_kim.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
      
      <a href="/assets/pdf/slides_mscthesis.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Phrase-based statistical machine translation (SMT) has a severe data sparsity prob- lem in computing translation probabilities. In this thesis, we present and analyze various usages of word classes to alleviate the sparsity and improve the translation quality of a phrase-based SMT system. First of all, we propose a novel smoothing formulation of phrase translation mod- els, using word classes on the word level within a phrase. Secondly, we modify the standard phrase-based decoder to utilize word class phrases as additional transla- tion options. Finally, we investigate the word alignments trained from word classes. The performance of our proposed approaches is measured on three different trans- lation tasks to prove their broad applicability. The experiments show that our smoothed translation model is comparable to the state-of-the-art word class models with a smaller number of features. In addition, our modified decoder significantly reduces the out-of-vocabulary rate and enhances the overall translation quality in both automatic metrics and manual evaluation. We also make an extensive comparison among different word class mappings in terms of their performance in phrase-based SMT. Our results reveal that only the number of classes affects the translation quality of our proposed methods, regard- less of the type of clustering algorithms and other parameters for estimating word classes.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2013</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="hassani2013subspace" class="col-sm-8">
    
      <div class="title">Subspace MOA: Subspace Stream Clustering Evaluation Using the MOA Framework</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Marwan Hassani,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  and Thomas Seidl
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the 18th International Conference on Database Systems for Advanced Applications (DASFAA 2013)</em>
      
      </div>

      <div class="periodical">
      
        <b>Best Demo Award Runner-up</b>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://link.springer.com/chapter/10.1007%2F978-3-642-37450-0_33" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
      <a href="https://github.com/matthhan/subspacemoa" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Most available static data are becoming more and more high-dimensional. Therefore, subspace clustering, which aims at finding clusters not only within the full dimension but also within subgroups of dimensions, has gained a significant importance. Recently, OpenSubspace framework was proposed to evaluate and explorate subspace clustering algorithms in WEKA with a rich body of most state of the art subspace clustering algorithms and measures. Parallel to it, MOA (Massive Online Analysis) framework was developed also above WEKA to provide algorithms and evaluation methods for mining tasks on evolving data streams over the full space only. Similar to static data, most streaming data sources are becoming high-dimensional, and tracking their evolving clusters is also becoming important and challenging. In this demonstrator, we present, to the best of our knowledge, the first subspace clustering evaluation framework over data streams called Subspace MOA. Our demonstrator follows the online-offline model which is used in most data stream clustering algorithms. In the online phase, users have the possibility to select one of three most famous summarization techniques to form the microclusters. In the offline phase, one of five subspace clustering algorithms can be selected. The framework is supported with a subspace stream generator, a visualization interface to present the evolving clusters over different subspaces, and various subspace clustering evaluation measures.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="hassani2013effective" class="col-sm-8">
    
      <div class="title">Effective Evaluation Measures for Subspace Clustering of Data Streams</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Marwan Hassani,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Yunsu Kim</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Seungjin Choi,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Thomas Seidl
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the Second Workshop on Quality Issues, Measures of Interestingness and Evaluation of Data Mining Models (QIMIE 2013)</em>
      
      </div>

      <div class="periodical">
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://link.springer.com/chapter/10.1007%2F978-3-642-40319-4_30" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Nowadays, most streaming data sources are becoming high-dimensional. Accordingly, subspace stream clustering, which aims at finding evolving clusters within subgroups of dimensions, has gained a significant importance. However, existing subspace clustering evaluation measures are mainly designed for static data, and cannot reflect the quality of the evolving nature of data streams. On the other hand, available stream clustering evaluation measures care only about the errors of the full-space clustering but not the quality of subspace clustering. In this paper we propose, to the first of our knowledge, the first subspace clustering measure that is designed for streaming data, called SubCMM: Subspace Cluster Mapping Measure. SubCMM is an effective evaluation measure for stream subspace clustering that is able to handle errors caused by emerging, moving, or splitting subspace clusters. Additionally, we propose a novel method for using available offline subspace clustering measures for data streams within the Subspace MOA framework.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2022 Yunsu  Kim.
    
    
    
    Last updated: May 24, 2022.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
